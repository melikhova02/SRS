---
title: "Математическое моделирование"
author: "Мелихова И.С."
date: '31 марта 2019 г '
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---
Данные: Online_Shopping_for_models.csv

```{r}
library('GGally')       # графики совместного разброса переменных
library('lmtest') # тесты остатков регрессионных моделей
library('FNN') # алгоритм kNN
library('mlbench')
library('MASS')
library('ISLR')
library('boot')
DF <- read.table('Online_Shopping_for_models.csv', header = T,            # заголовок в первой строке
                 dec = ',',             # разделитель целой и дробной части
                 sep = ';')     # символы пропущенных значений
df <- na.omit(DF)
dim(df)
head(df)
str(df)
my.seed <- 12  
set.seed(12)
set.seed(my.seed)
summary(df)
```

Выдвинем предположение о том, что выручка интернет-магазина будет зависить от показателя отказов, руководства, браузера и соответствующей продукции.


## Метод проверочной выборки 

Он состоит в том, что мы отбираем одну тестовую выборку и будем считать на ней ошибку модели.    

```{r}
# общее число наблюдений
n <- nrow(df)
# доля обучающей выборки
train.percent <- 0.5
# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(n, n * train.percent)
```

Построим модели для проверки точности. Вид моделей:  

**Линейная модель**: $\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot BounceRates+\hat{\beta}_2 \cdot Administrative+\hat{\beta}_3 \cdot Browser+\hat{\beta}_4 \cdot ProductRelated$. 

``` {r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm <- lm(Revenue ~ BounceRates +  Administrative + Browser + ProductRelated, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

```{r echo = F}
err.test <- mean((df$Revenue[-inTrain] - predict(fit.lm,
                              df[-inTrain, ]))^2)
names(err.test) <- 1
```


Построим **квадратичные модели** и оценим MSE при увеличении степеней объясняющих переменных.

Модели:

1)$\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot BounceRates^2+\hat{\beta}_2 \cdot Administrative+\hat{\beta}_3 \cdot Browser+\hat{\beta}_4 \cdot ProductRelated$

2)$\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot BounceRates+\hat{\beta}_2 \cdot Administrative^2+\hat{\beta}_3 \cdot Browser+\hat{\beta}_4 \cdot ProductRelated$

3)$\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot BounceRates+\hat{\beta}_2 \cdot Administrative+\hat{\beta}_3 \cdot Browser^2+\hat{\beta}_4 \cdot ProductRelated$

4)$\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot BounceRates+\hat{\beta}_2 \cdot Administrative+\hat{\beta}_3 \cdot Browser+\hat{\beta}_4 \cdot ProductRelated^2$   

Модель 1:

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm.1 <- lm(Revenue ~ poly(BounceRates,2) +  Administrative + Browser + ProductRelated, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm.1,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

```{r echo = F}
err.test <- c(err.test, 
              mean((df$Revenue[-inTrain] - predict(fit.lm.1,
                                                 df[-inTrain, ]))^2))
names(err.test)[length(err.test)] <- 2
```


Модель 2:

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm.2 <- lm(Revenue ~ BounceRates +  poly(Administrative,2) + Browser + ProductRelated, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm.2,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

```{r echo = F}
err.test <- c(err.test, 
              mean((Auto$mpg[-inTrain] - predict(fit.lm.2,
                                                 df[-inTrain, ]))^2))
names(err.test)[length(err.test)] <- 2
```

Модель 3:

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm.3 <- lm(Revenue ~ BounceRates +  Administrative + poly(Browser,2) + ProductRelated, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm.3,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

```{r echo = F}
err.test <- c(err.test, 
              mean((df$Revenue[-inTrain] - predict(fit.lm.3,
                                                 df[-inTrain, ]))^2))
names(err.test)[length(err.test)] <- 2
```

Модель 4:

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm.4 <- lm(Revenue ~ BounceRates +  Administrative + Browser + poly(ProductRelated,2), 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm.4,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

```{r echo = F}
err.test <- c(err.test, 
              mean((df$Revenue[-inTrain] - predict(fit.lm.4,
                                                 df[-inTrain, ]))^2))
names(err.test)[length(err.test)] <- 2
```

Ошибка модели при изменении степени каждого из регрессора практически не меняется.За наилучшую примем первоначальную линейную модель.Увеличивать степени дальше нет необходимости, так как уже сейчас MSE показывает довольно низкий результат. 


Построим модель LDA. Построим матрицу неточностей, оценим чувствительность, специфичность, верность и с помощью этой модели сделаем прогнозы на прогнозных данных.

```{r}
DF1 <- read.table('Online_Shopping_for_forecast.csv', header = T,            # заголовок в первой строке
                 dec = ',',             # разделитель целой и дробной части
                 sep = ';')     # символы пропущенных значений
df1 <- na.omit(DF1)
Revenue <- c("TRUE", "FALSE")
df1 <- data.frame(df1, Revenue) 
Revenue <- as.factor(Revenue)
Факт <- df$Revenue
Факт <- Факт[1:616]
model.lda <- lda(Revenue ~ BounceRates +  Administrative + Browser + ProductRelated, data = df1)
model.lda
```


```{r}
p.lda <- predict(model.lda, df1, type = 'response')
Прогноз <- factor(ifelse(p.lda$posterior[, 'TRUE'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('FALSE', 'TRUE'))

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m
# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)
```

Отчёт по модели LDA содержит три раздела: априарные вероятности классов (Prior probabilities of groups), групповые средние объясняющих переменных (Group means) и коэффициенты линейной разделяющей границы (Coefficients of linear discriminants).   



## ROC-кривая для LDA

Построим график совместного изменения чувствительности и специфичности с изменением вероятности отсечения от 0 до 1 – ROC-кривую. Для примера возьмём модель LDA.

```{r, fig.width = 5, fig.height = 5, message = F, warning = F}

Факт <- df$Revenue

model.logit <- glm(Revenue ~ BounceRates +  Administrative + Browser + ProductRelated, data = df, family = 'binomial')
summary(model.logit)

p.logit <- predict(model.logit, df, type = 'response')
Прогноз <- factor(ifelse(p.logit > 0.5, 2, 1),
                  levels = c(1, 2),
                  labels = c('FALSE', 'TRUE'))

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m

model.lda <- lda(Revenue ~ BounceRates +  Administrative + Browser + ProductRelated, data = df)
model.lda


p.lda <- predict(model.lda, df, type = 'response')
Прогноз <- factor(ifelse(p.lda$posterior[, 'TRUE'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('FALSE', 'TRUE'))

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m

x <- NULL    # для (1 - SPC)
y <- NULL    # для TPR

# заготовка под матрицу неточностей
tbl <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) <- c('fact.FALSE', 'fact.TRUE')
colnames(tbl) <- c('predict.FALSE', 'predict.TRUE')

# вектор вероятностей для перебора
p.vector <- seq(0, 1, length = 501)

# цикл по вероятностям отсечения
for (p in p.vector){
  # прогноз
  Прогноз <- factor(ifelse(p.lda$posterior[, 'TRUE'] > p, 
                           2, 1),
                    levels = c(1, 2),
                    labels = c('FALSE', 'TRUE'))
  
  # фрейм со сравнением факта и прогноза
  df.compare <- data.frame(Факт = Факт, Прогноз = Прогноз)
  
  # заполняем матрицу неточностей
  tbl[1, 1] <- nrow(df.compare[df.compare$Факт == 'FALSE' & df.compare$Прогноз == 'FALSE', ])
  tbl[2, 2] <- nrow(df.compare[df.compare$Факт == 'TRUE' & df.compare$Прогноз == 'TRUE', ])
  tbl[1, 2] <- nrow(df.compare[df.compare$Факт == 'FALSE' & df.compare$Прогноз == 'TRUE', ])
  tbl[2, 1] <- nrow(df.compare[df.compare$Факт == 'TRUE' & df.compare$Прогноз == 'FALSE', ])
  
  # считаем характеристики
  TPR <- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1])
  y <- c(y, TPR)
  SPC <- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2])
  x <- c(x, 1 - SPC)
}
```

Рассмотрим изменение чувствительности модели при изменение границы отсечения с 0.5 до 0.2.

```{r, fig.width = 5, fig.height = 5, message = F, warning = F}
# строим ROC-кривую
par(mar = c(5, 5, 1, 1))
# кривая
plot(x, y, type = 'l', col = 'blue', lwd = 3,
     xlab = '(1 - SPC)', ylab = 'TPR', 
     xlim = c(0, 1), ylim = c(0, 1))
# прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)

# точка для вероятности 0.5
points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16)
text(x[p.vector == 0.5], y[p.vector == 0.5], 'p = 0.5', pos = 4)
# точка для вероятности 0.2
points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16)
text(x[p.vector == 0.2], y[p.vector == 0.2], 'p = 0.2', pos = 4)
```

Видно, что изменение границы отсечения с 0.5 до 0.2 увеличивает чувствительность модели почти в шесть раз, в то время как специфичность немного ухудшается.

Рассмотрим изменение чувствительности модели при изменение границы отсечения с 0.5 до 0.08.

```{r, fig.width = 5, fig.height = 5, message = F, warning = F}
# строим ROC-кривую
par(mar = c(5, 5, 1, 1))
# кривая
plot(x, y, type = 'l', col = 'blue', lwd = 3,
     xlab = '(1 - SPC)', ylab = 'TPR', 
     xlim = c(0, 1), ylim = c(0, 1))
# прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)

# точка для вероятности 0.5
points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16)
text(x[p.vector == 0.5], y[p.vector == 0.5], 'p = 0.5', pos = 4)
# точка для вероятности 0.08
points(x[p.vector == 0.08], y[p.vector == 0.08], pch = 16)
text(x[p.vector == 0.08], y[p.vector == 0.08], 'p = 0.08', pos = 4)
```

Видно, что изменение границы отсечения с 0.5 до 0.08 увеличивает чувствительность модели почти в восемнадцать раз, но в то же время специфичность ухудшается очень сильно.
